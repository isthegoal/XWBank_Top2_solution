{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码提前声明\n",
    "写代码坚持以下原则：\n",
    "\n",
    "    *保证高内聚、低耦合的特性，各小块之间均是相互独立，把每个部分集成在一起，随时可以当做不上分点注释掉\n",
    "    \n",
    "    *把基本步骤分解开来，数据处理，模型整理都当做可选的方案，整体的可维护性最优\n",
    "    \n",
    "    *记住代码的层次、可维护性 是首当其冲，有好的体系下来，就算再难的问题，也能按照大佬模式逐个突破。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前置思考方案\n",
    "    1.特征预处理\n",
    "       *补缺失：对重要度大的特征进行缺失值补充\n",
    "       *异常值去除\n",
    "    ......\n",
    "    2.单模型尝试\n",
    "       *lightGBM\n",
    "       *Catboost\n",
    "    ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################     引包   #####################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import operator\n",
    "import time\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一.数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------         1.数据读取        -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 160)\n",
      "(10000, 159)\n",
      "(10000, 159)\n",
      "训练集的特征列train： Index(['cust_id', 'cust_group', 'y', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6',\n",
      "       'x_7',\n",
      "       ...\n",
      "       'x_148', 'x_149', 'x_150', 'x_151', 'x_152', 'x_153', 'x_154', 'x_155',\n",
      "       'x_156', 'x_157'],\n",
      "      dtype='object', length=160)\n",
      "训练集的特征列test： Index(['cust_id', 'cust_group', 'x_1', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6',\n",
      "       'x_7', 'x_8',\n",
      "       ...\n",
      "       'x_149', 'x_150', 'x_151', 'x_152', 'x_153', 'x_154', 'x_155', 'x_156',\n",
      "       'x_157', 'y'],\n",
      "      dtype='object', length=160)\n",
      "(15000, 160)\n",
      "(10000, 160)\n",
      "(25000, 160)\n",
      "训练集的特征列： Index(['cust_group', 'cust_id', 'x_1', 'x_10', 'x_100', 'x_101', 'x_102',\n",
      "       'x_103', 'x_104', 'x_105',\n",
      "       ...\n",
      "       'x_91', 'x_92', 'x_93', 'x_94', 'x_95', 'x_96', 'x_97', 'x_98', 'x_99',\n",
      "       'y'],\n",
      "      dtype='object', length=160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python35\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 读取文件\n",
    "train_xy = pd.read_csv(\"./data/train_xy.csv\",header=0,sep=\",\")\n",
    "train_x = pd.read_csv(\"./data/train_x.csv\",header=0,sep=\",\")\n",
    "test_all = pd.read_csv(\"./data/test_all.csv\",header=0,sep=\",\")\n",
    "print(train_xy.shape)\n",
    "print(train_x.shape)\n",
    "print(test_all.shape)\n",
    "train = train_xy.copy()\n",
    "test = test_all.copy()\n",
    "test['y'] = -1\n",
    "print('训练集的特征列train：',train.columns)\n",
    "print('训练集的特征列test：',test.columns)\n",
    "# 合并一下train 和 test\n",
    "data = pd.concat([train,test],axis = 0) # train_xy，test_all索引上连接\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(data.shape)\n",
    "print('训练集的特征列：',data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------         2.删除高缺特征        -------------------------\n",
    "*此处也加入了对缺失统计特征的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有 19 个特征列的缺失值严重，超过0.950000 \n",
      "一共有 26 个特征列的缺失值严重，超过0.950000 \n",
      "缺失严重的特征：train =?= test------ False\n",
      "严重缺失的特征有 28 个。\n",
      "一共删除的特征有 28 个。\n",
      "['x_116', 'x_127', 'x_126', 'x_129', 'x_133', 'x_131', 'x_135', 'x_112', 'x_109', 'x_113', 'x_110', 'x_94', 'x_102', 'x_92', 'x_134', 'x_137', 'x_119', 'x_118', 'x_128', 'x_115', 'x_132', 'x_107', 'x_111', 'x_130', 'x_138', 'x_123', 'x_108', 'x_114']\n",
      "(25000, 132)\n",
      "(15000, 132)\n",
      "(10000, 132)\n",
      "训练集的特征列： Index(['cust_group', 'cust_id', 'x_1', 'x_10', 'x_100', 'x_101', 'x_103',\n",
      "       'x_104', 'x_105', 'x_106',\n",
      "       ...\n",
      "       'x_9', 'x_90', 'x_91', 'x_93', 'x_95', 'x_96', 'x_97', 'x_98', 'x_99',\n",
      "       'y'],\n",
      "      dtype='object', length=132)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 删除一些不必要的特征（噪音、缺失严重、单值、重复等）     删除掉训练集和测试集中同时缺失大于百分之95的\n",
    "# train ,test 分开分析 \n",
    "# 处理一下缺失值严重的特征列，删除\n",
    "def get_nan_feature(train,rate = 0.95):\n",
    "    total_num = train.shape[0]\n",
    "    train_nan_feats = []\n",
    "    for i in range(157):\n",
    "        feat = 'x_' + str(i+1)\n",
    "        nan_num = train.loc[train[feat]==-99,:].shape[0]\n",
    "        nan_rate = nan_num / float(total_num) \n",
    "        \n",
    "        if nan_rate == 1.0:                    # 只有nan\n",
    "            train_nan_feats.append(feat)\n",
    "        if nan_rate > rate:                    # 有缺失值 nan,而且缺失严重\n",
    "            if len(train[feat].unique()) == 2: # 只有nan + 一个其他值\n",
    "                train_nan_feats.append(feat)\n",
    "\n",
    "    print(\"一共有 %d 个特征列的缺失值严重，超过%f \"%(len(train_nan_feats),rate))\n",
    "    return train_nan_feats\n",
    "\n",
    "train_nan_feats = get_nan_feature(train)\n",
    "test_nan_feats = get_nan_feature(test)\n",
    "print(\"缺失严重的特征：train =?= test------\",np.all(train_nan_feats == test_nan_feats))\n",
    "\n",
    "# 对这些特征取并集:28个\n",
    "nan_feats = list(set(train_nan_feats) | set(test_nan_feats)) # 按照train | test的结果,并集，交集，A or B 都一样     并集方式的，其实可以多尝试\n",
    "print('严重缺失的特征有 %d 个。'%(len(nan_feats)))\n",
    "\n",
    "# 总的删除的特征（发现删重复的5个特征，效果不好，所以只删除28个缺失严重的特征,这是有尝试过得）\n",
    "drop_feats = nan_feats \n",
    "print('一共删除的特征有 %d 个。'%(len(drop_feats)))\n",
    "print(drop_feats)\n",
    "\n",
    "# 删除缺失值严重的特征列\n",
    "train = train.drop(drop_feats, axis = 1)\n",
    "test = test.drop(drop_feats, axis = 1)\n",
    "data = data.drop(drop_feats, axis = 1)\n",
    "print(data.shape)\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "# 删除的特征，全部是特征重要性=0的特征，所以删除 = 原始的特征（不影响精确效果，但是可以减少噪音）\n",
    "# 删除了x_92 , x_94 是数值型的，其他 24 个 全部是 类别型\n",
    "\n",
    "print('训练集的特征列：',data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------         3.对缺失特征做处理        -------------------------\n",
    "\n",
    "   *此处还附加了将数值型特征进行排序并归一化处理，相当于做成了排序特征\n",
    "   \n",
    "   *也附加了构建缺失统计特征，并对缺失统计进行onehot编码，让其更稳定\n",
    "\n",
    "   *挑选重要度大、缺失比较少的特征进行填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剩下的数值型特征： 93\n",
      "剩下的类别型特征： 36\n",
      "数值型特征为： ['x_96', 'x_97', 'x_98', 'x_99', 'x_100', 'x_101', 'x_103', 'x_104', 'x_105', 'x_106', 'x_117', 'x_120', 'x_121', 'x_122', 'x_124', 'x_125', 'x_136', 'x_139', 'x_140', 'x_141', 'x_142', 'x_143', 'x_144', 'x_145', 'x_146', 'x_147', 'x_148', 'x_149', 'x_150', 'x_151', 'x_152', 'x_153', 'x_154', 'x_155', 'x_156', 'x_157']\n",
      "数值型为：       x_96  x_97  x_98  x_99  x_100  x_101  x_103  x_104  x_105  x_106  ...    \\\n",
      "0        2     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "1        1     2     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "2        1     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "3        1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "4        2     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "5        2     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "6        1     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "7        1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "8        1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9        1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "10       1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "11       1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "12       1     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "13       1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "14       1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "15       1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "16       1     2     3     1      1      1    -99    -99    -99    -99  ...     \n",
      "17       2     2     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "18       1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "19       1     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "20       1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "21       2     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "22       2     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "23       1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "24       1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "25       2     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "26       1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "27       1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "28       2     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "29       1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "...    ...   ...   ...   ...    ...    ...    ...    ...    ...    ...  ...     \n",
      "9970     2     3     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9971     2     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9972     1     3     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9973     1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9974     1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9975     2     3     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9976     1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9977     1     2     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "9978     1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9979     1     2     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "9980     2     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9981     1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9982     1     2     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "9983     2     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "9984     2     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9985     2     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "9986     1     3     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9987     2     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "9988     1     2     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "9989     1     1     3     1      1      1    -99    -99    -99    -99  ...     \n",
      "9990     2     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9991     1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9992     1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9993     1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9994     1     2     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "9995     2     3     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9996     1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9997     1     1     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9998     1     2     2     1      1      1    -99    -99    -99    -99  ...     \n",
      "9999     1     1     1     1      1      1    -99    -99    -99    -99  ...     \n",
      "\n",
      "      x_148  x_149  x_150  x_151  x_152  x_153  x_154  x_155  x_156  x_157  \n",
      "0         1      1      1      1      1      1      1      1      3    -99  \n",
      "1         1      1      1      1      1      1      1      1      2      2  \n",
      "2         1      1      2      1      1      1      1      1      2      2  \n",
      "3         2      1      1      1      1      1      1      1      2      4  \n",
      "4         1      1      1      1      1      1      1      1      2      1  \n",
      "5         1      1      1      1      1    -99      1      1      2    -99  \n",
      "6         1      1      1      1      1      1      1      1      2      4  \n",
      "7         1      1      1      1      1      1      1      1      3      2  \n",
      "8         1      1      1      1      1      1      1      1      2      3  \n",
      "9         1      1      1      1      1      1      1      1      1    -99  \n",
      "10        1      1      1      1      1      1      2      2    -99    -99  \n",
      "11        1      1      1      1      1      1      1      1      2      4  \n",
      "12        1      1      1      1      1      1      1      1      2      3  \n",
      "13        1      1      2      1      1      1      1      1      2      3  \n",
      "14        1      1      2      1      1      1      1      1      2      1  \n",
      "15        1      1      1      1      1      1      1      1      2      4  \n",
      "16        1      1      1      1      1      1      1      1      2      2  \n",
      "17        1      1      1      1      1      1      1      1      2      2  \n",
      "18        1      1      1      1      1      1      1      1      2      4  \n",
      "19        1      1      1      1      1      1      1      1      2      4  \n",
      "20        1      1      1      1      1      1      1      1      3    -99  \n",
      "21        1      1      1      1      1      1      1      1      2    -99  \n",
      "22        1      1      1      1      1      1      1      1      1    -99  \n",
      "23        1      3      2      1      1      1      1      1      2      4  \n",
      "24        1      1      1      1      1      1      1      1      2      4  \n",
      "25        1      1      1      1      1      1      1      1      2    -99  \n",
      "26        1      1      2      1      1      1      1      1      2      2  \n",
      "27        1      1      1      1      1      1      1      1      3      1  \n",
      "28        1      1      1      1      1      1      2      2      2      3  \n",
      "29        1      1      1      1      1      1      1      1      2    -99  \n",
      "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "9970      1      1      1      1      1      1      1      1      2      2  \n",
      "9971      1      1      1      1      1      1      2      2      3      1  \n",
      "9972      1      1      1      1      1      1      1      1      2      3  \n",
      "9973      1      1      1      1      1      1      1      1      2      3  \n",
      "9974      1      1      1      1      1      1      1      1      3      3  \n",
      "9975      1      1      1      1      1      1      2      2      2      3  \n",
      "9976      1      1      1      1      1      1      1      1      2      1  \n",
      "9977      1      1      1      1      1      1      1      1      3      1  \n",
      "9978      1      1      1      1      1      1      1      1      2      2  \n",
      "9979      1      1      1      1      1      1      1      1      1     10  \n",
      "9980      1      1      1      1      1      1      1      1      3      3  \n",
      "9981      1      1      1      1      1      1      1      1      3      4  \n",
      "9982      1      1      1      1      1      1      1      1      1      3  \n",
      "9983      1      3      2      1      1      1      1      1      2      2  \n",
      "9984      1      1      2      1      1      1      1      1      2      4  \n",
      "9985      1      1      1      1      1      1      1      1      3      2  \n",
      "9986      1      1      1      1      1      1      1      1      2      3  \n",
      "9987      1      1      1      1      1      1      1      1      2      3  \n",
      "9988      1      1      1      1      1      1      1      1      2      4  \n",
      "9989      1      1      1      1      1      1      1      1      1     10  \n",
      "9990      1      3      2      1      1      1      1      1      2      3  \n",
      "9991      1      1      1      1      1      1      2      2      1     11  \n",
      "9992      1      1      1      1      1      1      1      1      2      2  \n",
      "9993      1      1      1      1      1      1      2      2      1     11  \n",
      "9994      1      1      1      1      1      1      1      1      2      4  \n",
      "9995      1      1      1      1      1      1      2      2      2      1  \n",
      "9996      1      1      2      1      1      1      1      1      2      4  \n",
      "9997      1      1      1      1      1      1      1      1      2      2  \n",
      "9998      1      1      1      1      1      1      1      1      3      3  \n",
      "9999      1      1      1      1      1      1      2      2      2      3  \n",
      "\n",
      "[25000 rows x 36 columns]\n",
      "训练集的特征列： Index(['cust_group', 'cust_id', 'x_1', 'x_10', 'x_100', 'x_101', 'x_103',\n",
      "       'x_104', 'x_105', 'x_106',\n",
      "       ...\n",
      "       'x_9', 'x_90', 'x_91', 'x_93', 'x_95', 'x_96', 'x_97', 'x_98', 'x_99',\n",
      "       'y'],\n",
      "      dtype='object', length=132)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 对剩下的特征进行分析，分为数值型 、 类别型        (这里有疑问，难道默认0-94列特征是数值型特征吗，这很不合理啊)\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "for i in range(157):\n",
    "    feat = \"x_\" + str(i+1)\n",
    "    if feat not in drop_feats:\n",
    "        if i <= 94: # 1-95\n",
    "            numerical_features.append(feat)\n",
    "        else:\n",
    "            categorical_features.append(feat)\n",
    "print(\"剩下的数值型特征：\",len(numerical_features))\n",
    "print(\"剩下的类别型特征：\",len(categorical_features))\n",
    "\n",
    "print('数值型特征为：',categorical_features)\n",
    "print('数值型为：',data[categorical_features])\n",
    "print('训练集的特征列：',data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总列数: 132\n",
      "每行非空列数: [ 84  85 120 ... 117  82  85]\n",
      "每行空列数: 0       48\n",
      "1       47\n",
      "2       12\n",
      "3       11\n",
      "4       29\n",
      "5       54\n",
      "6       15\n",
      "7       51\n",
      "8       11\n",
      "9       48\n",
      "10      32\n",
      "11      11\n",
      "12      30\n",
      "13      50\n",
      "14      15\n",
      "15      33\n",
      "16      15\n",
      "17      33\n",
      "18      51\n",
      "19      51\n",
      "20      12\n",
      "21      48\n",
      "22      12\n",
      "23      13\n",
      "24      15\n",
      "25      52\n",
      "26      12\n",
      "27      12\n",
      "28      29\n",
      "29      15\n",
      "        ..\n",
      "9970    15\n",
      "9971    15\n",
      "9972    14\n",
      "9973    51\n",
      "9974    51\n",
      "9975    12\n",
      "9976    11\n",
      "9977    12\n",
      "9978    48\n",
      "9979    16\n",
      "9980    15\n",
      "9981    11\n",
      "9982    15\n",
      "9983    12\n",
      "9984    51\n",
      "9985    51\n",
      "9986    15\n",
      "9987    13\n",
      "9988    14\n",
      "9989    50\n",
      "9990    14\n",
      "9991    51\n",
      "9992    12\n",
      "9993    48\n",
      "9994    15\n",
      "9995    14\n",
      "9996    11\n",
      "9997    15\n",
      "9998    50\n",
      "9999    47\n",
      "Name: nan_count, Length: 25000, dtype: int64\n",
      "(25000, 7)\n",
      "(25000, 139)\n",
      "训练集的特征列： Index(['cust_group', 'cust_id', 'x_1', 'x_10', 'x_100', 'x_101', 'x_103',\n",
      "       'x_104', 'x_105', 'x_106',\n",
      "       ...\n",
      "       'x_98', 'x_99', 'y', 'nan_(7.895, 23.0]', 'nan_(23.0, 38.0]',\n",
      "       'nan_(38.0, 53.0]', 'nan_(53.0, 68.0]', 'nan_(68.0, 83.0]',\n",
      "       'nan_(83.0, 98.0]', 'nan_(98.0, 113.0]'],\n",
      "      dtype='object', length=139)\n"
     ]
    }
   ],
   "source": [
    "# 统计每个样本缺失值的个数                  统计缺失样本数 构建统计特征\n",
    "def get_nan_count(data,feats,bins = 7):\n",
    "    df = data[feats].copy()\n",
    "    df = df.replace(-99,np.nan)\n",
    "    print('总列数:',df.shape[1])   #这列展示了每一行缺失的特征的数量\n",
    "    print('每行非空列数:',df.count(axis = 1).values)   #这列展示了每一行缺失的特征的数量\n",
    "    df['nan_count'] = df.shape[1] - df.count(axis = 1).values  # 列数 - 非nan数\n",
    "    print('每行空列数:',df['nan_count'])   #这列展示了每一行缺失的特征的数量\n",
    "    dummy = pd.get_dummies(pd.cut(df['nan_count'],bins),prefix = 'nan') #把每行空列数，做7分离散化再转one-hot编码 对缺失数据进行离散化,划分为7个区间,对于划分区间，这里根据空值情况来造dummies特征\n",
    "    print(dummy.shape)\n",
    "    res = pd.concat([data,dummy],axis = 1) # 合并到原来的数据\n",
    "    print(res.shape)\n",
    "    return res\n",
    "# 在全部特征上面统计缺失值      新加入了对缺失值统计的7列\n",
    "data = get_nan_count(data,data.columns.values,7)\n",
    "print('训练集的特征列：',data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一共有 55 个特征列的缺失值较少，低于0.100000 \n",
      "缺失较少的特征： ['x_1', 'x_2', 'x_39', 'x_40', 'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58', 'x_59', 'x_60', 'x_61', 'x_62', 'x_63', 'x_64', 'x_65', 'x_66', 'x_67', 'x_68', 'x_69', 'x_70', 'x_71', 'x_72', 'x_73', 'x_74', 'x_75', 'x_76', 'x_77', 'x_78', 'x_79', 'x_80', 'x_81', 'x_82', 'x_83', 'x_84', 'x_85', 'x_86', 'x_87', 'x_88', 'x_89', 'x_90', 'x_91']\n",
      "训练集的特征列： Index(['cust_group', 'cust_id', 'x_1', 'x_10', 'x_100', 'x_101', 'x_103',\n",
      "       'x_104', 'x_105', 'x_106',\n",
      "       ...\n",
      "       'x_98', 'x_99', 'y', 'nan_(7.895, 23.0]', 'nan_(23.0, 38.0]',\n",
      "       'nan_(38.0, 53.0]', 'nan_(53.0, 68.0]', 'nan_(68.0, 83.0]',\n",
      "       'nan_(83.0, 98.0]', 'nan_(98.0, 113.0]'],\n",
      "      dtype='object', length=139)\n",
      "训练集的特征列： Index(['cust_group', 'cust_id', 'x_1', 'x_10', 'x_100', 'x_101', 'x_103',\n",
      "       'x_104', 'x_105', 'x_106',\n",
      "       ...\n",
      "       'x_98', 'x_99', 'y', 'nan_(7.895, 23.0]', 'nan_(23.0, 38.0]',\n",
      "       'nan_(38.0, 53.0]', 'nan_(53.0, 68.0]', 'nan_(68.0, 83.0]',\n",
      "       'nan_(83.0, 98.0]', 'nan_(98.0, 113.0]'],\n",
      "      dtype='object', length=139)\n"
     ]
    }
   ],
   "source": [
    "# 获取缺失很少的数值型的特征         缺失少的数值型用均值\n",
    "def get_little_nan_feats(df,numerical_features,rate = 0.1):\n",
    "    total_num = df.shape[0]\n",
    "    little_nan_feats = []\n",
    "    for feat in numerical_features:\n",
    "        nan_num = df.loc[df[feat]==-99,:].shape[0]\n",
    "        nan_rate = nan_num / float(total_num) \n",
    "        if nan_rate <= rate:\n",
    "            little_nan_feats.append(feat)\n",
    "            #print(\"feature:\",feat,\"nan_num = \",nan_num,\"nan_rate = \",nan_rate)\n",
    "    print(\"一共有 %d 个特征列的缺失值较少，低于%f \"%(len(little_nan_feats),rate))\n",
    "    return little_nan_feats\n",
    "little_nan_feats = get_little_nan_feats(data, numerical_features)\n",
    "print(\"缺失较少的特征：\",little_nan_feats)\n",
    "\n",
    "print('训练集的特征列：',data.columns)\n",
    "'''\n",
    "# 对一些数值的特征进行平均值填充\n",
    "for feat in little_nan_feats:\n",
    "    data[feat] = data[feat].replace(-99,np.nan)\n",
    "    data[feat] = data[feat].fillna(data[feat].mean())\n",
    "'''# 对一些重要数值的特征进行平均值填充    (根据其特性吧，因为不是时序问题，线性差值肯定是不合适的。)\n",
    "#对重要特征进行填充\n",
    "for feat in ['x_81','x_95']:\n",
    "    data[feat] = data[feat].replace(-99,np.nan)\n",
    "    data[feat] = data[feat].fillna(data[feat].mean())\n",
    "\n",
    "\n",
    "# 对数值型的特征，处理为rank特征（鲁棒性好一点）     数值本身就代表大小的意思，这里构建排序特征并进行归一化，效果会更加鲁棒一些。\n",
    "\n",
    "for feat in numerical_features:\n",
    "    #print('rank前：',data[feat])\n",
    "    data[feat] = data[feat].rank() / float(data.shape[0]) # 排序，并且进行归一化        这样也行？\n",
    "    #print('rank后：',data[feat])\n",
    "print('训练集的特征列：',data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "假定的重要特征个数为： 17\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "进行填充吧\n",
      "缺失数量为： 0\n",
      "缺失数量为： 0\n",
      "缺失数量为： 0\n",
      "缺失数量为： 0\n",
      "缺失数量为： 0\n",
      "缺失数量为： 0\n"
     ]
    }
   ],
   "source": [
    "#对重要的特征进行填充，之后还会想着对着重要的特征利用其进行采样操作\n",
    "imp_feat=['x_80','x_2','x_81','x_95','x_1','x_52','x_63','x_54','x_43','x_40','x_93','x_42','x_157','x_62','x_29','x_61','x_55']\n",
    "print('假定的重要特征个数为：',len(imp_feat))\n",
    "for feat in imp_feat[:10]:\n",
    "    if feat in numerical_features:\n",
    "        print('进行填充吧')\n",
    "        data[feat]=data[feat].replace(-99,np.nan)\n",
    "        data[feat]=data[feat].fillna(data[feat].mean())\n",
    "    if feat in categorical_features:\n",
    "        print('这是类别特征：',feat)\n",
    "        \n",
    "        \n",
    "#确认下这些特征是否已经被填充好了\n",
    "for feat in ['x_80','x_2','x_81','x_95','x_1','x_52']:\n",
    "    nan_num = data.loc[data[feat]==-99,:].shape[0]\n",
    "    print('缺失数量为：',nan_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------         4.数据采样处理        -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这个方式可能效果不太好\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE       #过度抽样处理库SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# #进行两种采样方式的处理，欠采样和过采样的处理，尝试在重要的10个特征上面进行采样，来获得标签\n",
    "# #过采样处理\\\n",
    "# print('开始了,第一种方式(可选择：选择计算的距离依据是最重要的哪几个特征？ 往往更多的特征依据会起到更好的采样生成效果，可以多做尝试吧 )')\n",
    "# x= data[['x_80','x_2','x_81','x_95','x_1','x_52','x_63','x_54','x_43']]\n",
    "# feature_columns=[i for i in x.columns]\n",
    "# y =data['y']\n",
    "# #groupby_data_orginal = y.groupby('label').count()\n",
    "# #print(x)\n",
    "# #print(y)\n",
    "# model_smote = SMOTE()  # 建立smote模型对象\n",
    "# x_smote_resampled, y_smote_resampled = model_smote.fit_sample(x, y)\n",
    "# x_smote_resampled = pd.DataFrame(x_smote_resampled, columns=feature_columns)\n",
    "# y_smote_resampled = pd.DataFrame(y_smote_resampled, columns=['y'])\n",
    "# smote_resampled = pd.concat([x_smote_resampled, y_smote_resampled], axis=1)\n",
    "# groupby_data_smote = smote_resampled.groupby('y').count()\n",
    "# smote_resampled.to_csv('./feature_data/经过过采样后的数据.csv')\n",
    "# print(smote_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二.特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------         1.特征整理      -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 139)\n",
      "(10000, 139)\n",
      "训练集的特征列： Index(['cust_group', 'cust_id', 'x_1', 'x_10', 'x_100', 'x_101', 'x_103',\n",
      "       'x_104', 'x_105', 'x_106',\n",
      "       ...\n",
      "       'x_98', 'x_99', 'y', 'nan_(7.895, 23.0]', 'nan_(23.0, 38.0]',\n",
      "       'nan_(38.0, 53.0]', 'nan_(53.0, 68.0]', 'nan_(68.0, 83.0]',\n",
      "       'nan_(83.0, 98.0]', 'nan_(98.0, 113.0]'],\n",
      "      dtype='object', length=139)\n",
      "所有特征的维度： 136\n"
     ]
    }
   ],
   "source": [
    "######################################################      1.特征整理   #############################################\n",
    "\n",
    "train = data.loc[data['y']!=-1,:] # train set\n",
    "test = data.loc[data['y']==-1,:]  # test set\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "#print(train)\n",
    "print('训练集的特征列：',train.columns)\n",
    "# 获取特征列，去除id，group,y\n",
    "no_features = ['cust_id','cust_group','y'] \n",
    "features = [feat for feat in train.columns.values if feat not in no_features]\n",
    "print(\"所有特征的维度：\",len(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------         2.构建交叉特征       -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################        #############################################\n",
    "\n",
    "#使用找到的还不错的特征，但是发现线上效果并不好，所以就不用了吧\n",
    "\n",
    "#多项式特征，二次变换特征    变换特征基本没什么用\n",
    "# train['all_index']=[i for i in range(0,15000)]\n",
    "# train['x_80-x_63']=train['x_80']-train['x_63']\n",
    "# train['x_80+x_64']=train['x_80']+train['x_64']\n",
    "# train['x_80/x_63']=train['x_80']/train['x_63']\n",
    "\n",
    "# train['x_95+x_93']=train['x_95']+train['x_42']\n",
    "# train['x_80+x_93']=train['x_80']+train['x_42']\n",
    "# train['x_63*x_93']=train['x_95']-train['x_42']\n",
    "# train['x_63**2']=train['x_80']**1/2\n",
    "\n",
    "\n",
    "# #test['all_index']=[i for i in range(0,10000)]\n",
    "\n",
    "# test['x_95+x_93']=test['x_95']+test['x_42']\n",
    "# test['x_80+x_93']=test['x_80']+test['x_42']\n",
    "# test['x_63*x_93']=test['x_95']-test['x_42']\n",
    "# test['x_63**2']=test['x_80']**1/2\n",
    "\n",
    "# test['x_80-x_63']=test['x_80']-test['x_63']\n",
    "# test['x_80+x_64']=test['x_80']+test['x_64']\n",
    "# test['x_80/x_63']=test['x_80']/test['x_63']\n",
    "\n",
    "# #这里，之前交叉特征根本没加入到训练里面尴尬\n",
    "# features.extend(['x_95+x_93','x_80+x_93','x_63*x_93','x_63**2','x_80-x_63','x_80+x_64','x_80/x_63'])\n",
    "# print(train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------         3.特征分离       -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X features : ['x_1', 'x_10', 'x_100', 'x_101', 'x_103', 'x_104', 'x_105', 'x_106', 'x_11', 'x_117', 'x_12', 'x_120', 'x_121', 'x_122', 'x_124', 'x_125', 'x_13', 'x_136', 'x_139', 'x_14', 'x_140', 'x_141', 'x_142', 'x_143', 'x_144', 'x_145', 'x_146', 'x_147', 'x_148', 'x_149', 'x_15', 'x_150', 'x_151', 'x_152', 'x_153', 'x_154', 'x_155', 'x_156', 'x_157', 'x_16', 'x_17', 'x_18', 'x_19', 'x_2', 'x_20', 'x_21', 'x_22', 'x_23', 'x_24', 'x_25', 'x_26', 'x_27', 'x_28', 'x_29', 'x_3', 'x_30', 'x_31', 'x_32', 'x_33', 'x_34', 'x_35', 'x_36', 'x_37', 'x_38', 'x_39', 'x_4', 'x_40', 'x_41', 'x_42', 'x_43', 'x_44', 'x_45', 'x_46', 'x_47', 'x_48', 'x_49', 'x_5', 'x_50', 'x_51', 'x_52', 'x_53', 'x_54', 'x_55', 'x_56', 'x_57', 'x_58', 'x_59', 'x_6', 'x_60', 'x_61', 'x_62', 'x_63', 'x_64', 'x_65', 'x_66', 'x_67', 'x_68', 'x_69', 'x_7', 'x_70', 'x_71', 'x_72', 'x_73', 'x_74', 'x_75', 'x_76', 'x_77', 'x_78', 'x_79', 'x_8', 'x_80', 'x_81', 'x_82', 'x_83', 'x_84', 'x_85', 'x_86', 'x_87', 'x_88', 'x_89', 'x_9', 'x_90', 'x_91', 'x_93', 'x_95', 'x_96', 'x_97', 'x_98', 'x_99', 'nan_(7.895, 23.0]', 'nan_(23.0, 38.0]', 'nan_(38.0, 53.0]', 'nan_(53.0, 68.0]', 'nan_(68.0, 83.0]', 'nan_(83.0, 98.0]', 'nan_(98.0, 113.0]']\n",
      "X shape: (15000, 136)\n",
      "y shape: (15000,)\n",
      "test shape (10000, 136)\n",
      "Index(['cust_group', 'x_1', 'x_10', 'x_100', 'x_101', 'x_103', 'x_104',\n",
      "       'x_105', 'x_106', 'x_11',\n",
      "       ...\n",
      "       'x_98', 'x_99', 'y', 'nan_(7.895, 23.0]', 'nan_(23.0, 38.0]',\n",
      "       'nan_(38.0, 53.0]', 'nan_(53.0, 68.0]', 'nan_(68.0, 83.0]',\n",
      "       'nan_(83.0, 98.0]', 'nan_(98.0, 113.0]'],\n",
      "      dtype='object', length=138)\n",
      "训练集的特征列： Index(['cust_group', 'x_1', 'x_10', 'x_100', 'x_101', 'x_103', 'x_104',\n",
      "       'x_105', 'x_106', 'x_11',\n",
      "       ...\n",
      "       'x_98', 'x_99', 'y', 'nan_(7.895, 23.0]', 'nan_(23.0, 38.0]',\n",
      "       'nan_(38.0, 53.0]', 'nan_(53.0, 68.0]', 'nan_(68.0, 83.0]',\n",
      "       'nan_(83.0, 98.0]', 'nan_(98.0, 113.0]'],\n",
      "      dtype='object', length=138)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 得到输入X ，输出y\n",
    "#train=train.replace(-99,np.nan)#  尝试进行替换之后再训练看看效果（发现效果变差了）\n",
    "#test=test.replace(-99,np.nan)#  尝试进行替换之后再训练看看效果（发现效果变差了）\n",
    "train_id = train.pop('cust_id')\n",
    "y = train['y'].values\n",
    "X = train[features].values\n",
    "print('X features :',features)\n",
    "print(\"X shape:\",X.shape)\n",
    "print(\"y shape:\",y.shape)\n",
    "\n",
    "test_id = test.pop('cust_id')\n",
    "test_data = test[features].values\n",
    "print(\"test shape\",test_data.shape)\n",
    "print(train.columns)\n",
    "train.to_csv('./feature_data/经过特征工程后的整个表.csv')\n",
    "# X.to_csv('./feature_data/train_feature.csv')\n",
    "# y.to_csv('./feature_data/train_label.csv')\n",
    "print('训练集的特征列：',train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三.单模型方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------         1.Xgboost模型       -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start：********************************\n",
      "valid auc: 0.8148273833763178\n",
      "valid auc: 0.7920578494819676\n",
      "valid auc: 0.8613744315822521\n",
      "valid auc: 0.836001985031244\n",
      "valid auc: 0.7894928802638178\n",
      "the cv information:\n",
      "[0.8148273833763178, 0.7920578494819676, 0.8613744315822521, 0.836001985031244, 0.7894928802638178]\n",
      "cv mean score 0.8187509059471199\n",
      "......................run with time:  0.7499267260233561\n",
      "over:*********************************\n"
     ]
    }
   ],
   "source": [
    "#开始xgb的cv训练\n",
    "# 采取分层采样\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"start：********************************\")\n",
    "start = time.time()\n",
    "\n",
    "N = 5\n",
    "skf = StratifiedKFold(n_splits=N,shuffle=True,random_state=2018)\n",
    "\n",
    "auc_cv = []\n",
    "pred_cv = []\n",
    "for k,(train_in,test_in) in enumerate(skf.split(X,y)):\n",
    "    X_train,X_test,y_train,y_test = X[train_in],X[test_in],\\\n",
    "                                    y[train_in],y[test_in]    \n",
    "    # 模型参数\n",
    "    params = {\t'booster':'gbtree',\n",
    "\t'objective': 'binary:logistic',\n",
    "\t'early_stopping_rounds':100,\n",
    "#     'gamma':0,#0.2 is ok\n",
    "#     'max_depth':8,\n",
    "# # \t'lambda':550,\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':0.3,\n",
    "    'min_child_weight':2.5, \n",
    "    'eta': 0.007,\n",
    "\t'seed':2018,\n",
    "\t'nthread':7}\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvali = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=800)\n",
    "    predict = model.predict(dvali)\n",
    "    tmp_auc = roc_auc_score(y_test,predict)\n",
    "    auc_cv.append(tmp_auc)\n",
    "    print(\"valid auc:\",tmp_auc)\n",
    "    # test\n",
    "    pred = model.predict(xgb.DMatrix(test_data))\n",
    "    pred_cv.append(pred) \n",
    "    \n",
    "    \n",
    "#     xgb.plot_importance(model,max_num_features=20)\n",
    "#     plt.title('xgb Feature Importance')\n",
    "#     plt.xlabel('relative importance')\n",
    "#     plt.show()\n",
    "\n",
    "# K交叉验证的平均分数 \n",
    "print('the cv information:')\n",
    "print(auc_cv)\n",
    "print('cv mean score',np.mean(auc_cv))\n",
    "\n",
    "end = time.time()\n",
    "print(\"......................run with time: \",(end - start) / 60.0 )\n",
    "print(\"over:*********************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "线下效果记录：（记住那些特征能够在线上涨分就作为使用的特征）\n",
    "\n",
    "\n",
    "    [1]使用如下参数，cv效果是0.8095941001927958   (初版特征)\n",
    "              'booster': 'gbtree',    \n",
    "              'objective': 'binary:logistic',\n",
    "              'eta': 0.02,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,  # 0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 9,  # 2 3\n",
    "              'silent': 1\n",
    "    [2]默认参数下，cv值是：0.6694547520305475\n",
    "    \n",
    "    \n",
    "    [3]使用如下调参后的结果为0.8118425236460529   (初版特征)\n",
    "              'booster': 'gbtree',\n",
    "              'objective': 'binary:logistic',\n",
    "              'eta': 0.02,\n",
    "              'max_depth': 3,  # 4 3         (调出)\n",
    "              'colsample_bytree': 0.7,  # 0.8\n",
    "              'subsample': 0.5,              (调出)\n",
    "              'min_child_weight': 9,  # 2 3\n",
    "              'silent': 1\n",
    "    [4]使用如下调参后的结果为0.814163899785226    (初版特征)\n",
    "              'booster': 'gbtree',\n",
    "              'objective': 'binary:logistic',\n",
    "              'eta': 0.01,\n",
    "              'max_depth': 3,  # 4 3\n",
    "              'colsample_bytree': 0.5,  # 0.8\n",
    "              'subsample': 0.5,\n",
    "              'min_child_weight': 9,  # 2 3\n",
    "              'silent': 1,\n",
    "              'gamma': 0    \n",
    "     [5]使用模型4与lightgbm模型(0.81952101)进行blend （前期放弃融合方案，做单模型计算，并且每个模型都要充分考虑过拟合的问题）\n",
    "     0.7    0.3的权值分配  线下：0.8184459400734287     线上值：0.75182 （这个稳定性稍好点）\n",
    "     0.6    0.4的权值分配  线下：0.818504121445845\n",
    "     0.5    0.5的权值分配  线下：0.8182861386523573    \n",
    "     0.9    0.1的权值分配  线下：0.8175251371900479\n",
    "     与lighhtgbm模型（0.8211793803301299）结合线下：  \n",
    "     0.7    0.3的权值分配  线下：0.8210557752290836     线上值：0.750+效果变差了\n",
    "     [6]使用如下调参后的结果为0.8187509059471199    (初版特征)\n",
    "     'booster':'gbtree',\n",
    "\t'objective': 'binary:logistic',\n",
    "\t'early_stopping_rounds':100,\n",
    "    'subsample':0.7,\n",
    "    'colsample_bytree':0.3,\n",
    "    'min_child_weight':2.5, \n",
    "    'eta': 0.007,\n",
    "\t'seed':2018,\n",
    "\t'nthread':7\n",
    "     注意：与高cv的线下分进行融合效果变差，说明调参调的过拟合了。\n",
    "     ！！！！！！！！！！！！！！！！！！！！！！！！！！！！   不做模型调整了，   ！！！！！！！！！！！！！！！！！！！！\n",
    "     [7]把-99替换下为nan,让模型自己去判断空值 在线下有点提升为0.8190473478328284\n",
    "     [8]加入交叉特征，线下为0.8192969307017934  ，统计特征之间加入没效果，是自己加入方式不对，有问题，没有把列名实际加上去，程序逻辑写的出问题了。 线上有下降，那么交叉特征就干脆不用了\n",
    "       train['x_80-x_63']=train['x_80']-train['x_63']\n",
    "       train['x_80+x_64']=train['x_80']+train['x_64']\n",
    "       train['x_80/x_63']=train['x_80']/train['x_63']\n",
    "       train['x_95+x_93']=train['x_95']+train['x_42']\n",
    "       train['x_80+x_93']=train['x_80']+train['x_42']\n",
    "       train['x_63*x_93']=train['x_95']-train['x_42']\n",
    "       train['x_63**2']=train['x_80']**1/2\n",
    "   \n",
    "     [9]使用上序号，能够让线下效果提升到0.9842626010230955，但是这样根本就不行哎，会在线上完全过拟合的，(用统计特征会降分，又过拟合了)\n",
    "        train['all_index']=[i for i in range(0,15000)]\n",
    "        train['x_80-x_63']=train['x_80']-train['x_63']\n",
    "        train['x_80+x_64']=train['x_80']+train['x_64']\n",
    "        train['x_80/x_63']=train['x_80']/train['x_63']\n",
    "\n",
    "        train['x_95+x_93']=train['x_95']+train['x_42']\n",
    "        train['x_80+x_93']=train['x_80']+train['x_42']\n",
    "        train['x_63*x_93']=train['x_95']-train['x_42']\n",
    "        train['x_63**2']=train['x_80']**1/2\n",
    "     [10]原效果是0.8190，现在对最重要的10个特征进行填充，线下效果变成0.8190473478328284,发现并没有多大的效果，但是这种提分点可能是在xgboost中不敏感吧。\n",
    "        \n",
    "        \n",
    "参数包括：\n",
    "参考文献：https://blog.csdn.net/han_xiaoyang/article/details/52665396\n",
    "--------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean auc: 0.8187509059471199\n",
      "总的结果： (5, 10000)\n",
      "result shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#生成预测结果\n",
    "mean_auc = np.mean(auc_cv)\n",
    "print(\"mean auc:\",mean_auc)\n",
    "filepath = 'result/xgb_'+ str(mean_auc)+'.csv' # 线下平均分数\n",
    "\n",
    "# 转为array\n",
    "res =  np.array(pred_cv)\n",
    "print(\"总的结果：\",res.shape)\n",
    "\n",
    "# 最后结果，mean，max，min\n",
    "r = res.mean(axis = 0)\n",
    "print('result shape:',r.shape)\n",
    "\n",
    "result = DataFrame()\n",
    "result['cust_id'] = test_id\n",
    "result['pred_prob'] = r\n",
    "result.to_csv(filepath,index=False,sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb找参数（网格调参）\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "#使用网格搜索找最佳的参数\n",
    "param_grid = {'gamma':[0,0.1,0.2,0.3,],'learning_rate':[0.01,0.02,0.03,0.04,],'colsample_bytree':[0.5,0.6,0.7,0.8],'subsample':[0.5,0.6,0.7,0.8]}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(max_depth=3,\n",
    "    learning_rate =0.01, n_estimators=140,       \n",
    "    min_child_weight=1,objective= 'binary:logistic', nthread=4,seed=27),       \n",
    "    param_grid=param_grid,cv=5)\n",
    "gsearch1.fit(X,y)\n",
    "print(gsearch1.best_params_,gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "相比于xgboost中的方式，会有些参数名字上的修改，不过影响不大：\n",
    "    \n",
    "    1、eta -> learning_rate \n",
    "    2、lambda -> reg_lambda \n",
    "    3、alpha -> reg_alpha\n",
    "\n",
    "调参记录:\n",
    "\n",
    "    *boostergbtree 树模型做为基分类器（默认）\n",
    "    *gbliner 线性模型做为基分类器\n",
    "    *n_jobs  并行线程数\n",
    "    *silentsilent=0时，不输出中间过程（默认）silent=1时，输出中间过程\n",
    "    *nthreadnthread=-1时，使用全部CPU进行并行运算（默认）nthread=1时，使用1个CPU进行运算。\n",
    "    *scale_pos_weight正样本的权重，在二分类任务中，当正负样本比例失衡时，设置正样本的权重，模型效果更好。例如，当正负样本比例为1:10时scale_pos_weight=10。\n",
    "    *n_estimatores含义：总共迭代的次数，即决策树的个数调参：\n",
    "    *max_depth含义：树的深度，默认值为6，典型值3-10。调参：值越大，越容易过拟合；值越小，越容易欠拟合。\n",
    "    *min_child_weight含义：默认值为1,。调参：值越大，越容易欠拟合；值越小，越容易过拟合（值较大时，避免模型学习到局部的特殊样本）。\n",
    "    *subsample含义：训练每棵树时，使用的数据占全部训练集的比例。默认值为1，典型值为0.5-1。调参：防止overfitting。\n",
    "    *colsample_bytree含义：训练每棵树时，使用的特征占全部特征的比例。默认值为1，典型值为0.5-1。调参：防止overfitting。\n",
    "    *learning_rate含义：学习率，控制每次迭代更新权重时的步长，默认0.3。调参：值越小，训练越慢。典型值为0.01-0.2。\n",
    "    *gamma惩罚项系数，指定节点分裂所需的最小损失函数下降值。调参：\n",
    "    *alphaL1正则化系数，默认为1lambdaL2正则化系数，默认为1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四.模型融合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附录1：尝试点日志"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附录2：上分点日志"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
