{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#f = open('./处理过程中的文件/all_data_补充null计数特征.csv')\n",
    "f = open('./data/all_data_old.csv')\n",
    "all_data_raw = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_raw = pd.read_csv('./data/train_y.csv')\n",
    "train_y = train_y_raw['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_all_data(all_data_raw):\n",
    "    train_x_raw = all_data_raw.loc[all_data_raw['belong'] == 1,:]\n",
    "    train_unlabeled_raw = all_data_raw.loc[all_data_raw['belong'] == 2,:]\n",
    "    test_x_raw = all_data_raw.loc[all_data_raw['belong'] == 3,:]\n",
    "    return train_x_raw,train_unlabeled_raw,test_x_raw\n",
    "#train_x,train_unlabeled,test_x = split_all_data(all_data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape (15000, 157)\n",
      "test_shape (10000, 157)\n",
      "###################### train!!! ################################\n",
      "[0]\tvalidation_0-auc:0.720102\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.794419\n",
      "[200]\tvalidation_0-auc:0.798787\n",
      "[300]\tvalidation_0-auc:0.803119\n",
      "[400]\tvalidation_0-auc:0.806547\n",
      "[500]\tvalidation_0-auc:0.80893\n",
      "[600]\tvalidation_0-auc:0.810614\n",
      "Stopping. Best iteration:\n",
      "[562]\tvalidation_0-auc:0.811092\n",
      "\n",
      "###################### valid!!! ##################################\n",
      "The auc_score is 0.8110920068976264\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "[0]\tvalidation_0-auc:0.690017\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.786046\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-17a7d2ace688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./preds_xgb/avg_preds.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauc_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mxgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-17a7d2ace688>\u001b[0m in \u001b[0;36mxgb\u001b[1;34m(all_data, train_y)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m#                      }\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m#gsearch = GridSearchCV(estimator = model, param_grid = param_test, scoring='roc_auc', cv=5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m                \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                \u001b[0meval_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auc'\u001b[0m\u001b[1;33m,\u001b[0m                \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m                \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;31m#model.fit(x_train,y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[0;32m    322\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                               verbose_eval=verbose, xgb_model=xgb_model)\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1021\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1022\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def xgb(all_data,train_y):\n",
    "    \n",
    "    \n",
    "    if os.path.exists('preds_xgb') == True:\n",
    "        shutil.rmtree('preds_xgb')\n",
    "    os.mkdir('preds_xgb')\n",
    "    \n",
    "    train_x,train_unlabeled,test_x = split_all_data(all_data)\n",
    "    train_x=train_x.replace(np.nan,-99)\n",
    "    test_x = test_x.replace(np.nan,-99)\n",
    "    cust_id = test_x['cust_id']\n",
    "    auc_score = []\n",
    "    k = 5\n",
    "    skf = StratifiedKFold(n_splits=k,shuffle=True,random_state=1)\n",
    "    feats = [feature for feature in train_x.columns.values if feature not in ['cust_id','cust_group','y','belong']]\n",
    "    train_x = train_x[feats]\n",
    "    test_x = test_x[feats]\n",
    "    print('train_shape',train_x.shape)\n",
    "    print('test_shape',test_x.shape)\n",
    "    for k,(train_k,valid_k) in enumerate(skf.split(train_x,train_y)):\n",
    "        x_train,y_train,x_valid,y_valid = np.array(train_x)[train_k], np.array(train_y)[train_k], np.array(train_x)[valid_k], np.array(train_y)[valid_k]\n",
    "        print('###################### train!!! ################################')       \n",
    "        #model_xgb = XGBRegressor(n_estimators=700,max_depth=3,learning_rate=0.07, subsample=0.9,colsample_bytree=0.7)\n",
    "        model = XGBRegressor(learning_rate=0.01,\n",
    "                             n_estimators=10000,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=6,\n",
    "                             gamma=0,\n",
    "                             subsample=0.7,                  #这个参数控制对于每棵树，随机采样的比例。减小这个参数的值算法会更加保守，避免过拟合。\n",
    "                                                             #但是这个值设置的过小，它可能会导致欠拟合。典型值：0.5-1 \n",
    "                             colsample_bytree=0.7,           # 用来控制每颗树随机采样的列数的占比每一列是一个特征0.5-1\n",
    "                             objective= 'binary:logistic',\n",
    "                             reg_alpha = 0.01,\n",
    "                             seed=27)#XGBClassifierXGBRegressor\n",
    "#         param_test = {#'max_depth':list(range(3,30,3)), \n",
    "# #                       'min_child_weight':[list(range(2,10,2))],\n",
    "#                       'n_estimators': list(range(100,5000,500)),  \n",
    "#                       'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "#                       'gamma': [i/10.0 for i in range(0,5)],\n",
    "#                       'subsample':[i/10.0 for i in range(6,10)],\n",
    "#                       'colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "#                       'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "#                      }\n",
    "        #gsearch = GridSearchCV(estimator = model, param_grid = param_test, scoring='roc_auc', cv=5)\n",
    "        model.fit(x_train,y_train,\\\n",
    "                eval_set=[(x_valid, y_valid)],\\\n",
    "                eval_metric='auc',\\\n",
    "                early_stopping_rounds=100,\\\n",
    "                verbose = 100)\n",
    "        #model.fit(x_train,y_train)\n",
    "        \n",
    "        print('###################### valid!!! ##################################')\n",
    "        y_pred_val = model.predict(x_valid)\n",
    "        auc = roc_auc_score(y_valid,y_pred_val)\n",
    "        print('The auc_score is',auc)\n",
    "        auc_score.append(auc)\n",
    "        print('###################### predict!!! ################################')\n",
    "        test_pred_y = model.predict(np.array(test_x))\n",
    "        test_result = pd.DataFrame(columns=[\"cust_id\",\"pred_prob\"])\n",
    "        test_result.cust_id = cust_id\n",
    "        test_result.pred_prob = test_pred_y\n",
    "        test_result.to_csv(\"./preds_xgb/xgb{0}.csv\".format(k),index=None,encoding='utf-8')\n",
    "    #pred 取平均   \n",
    "    files = os.listdir('./preds_xgb')\n",
    "    pred = pd.read_csv('./preds_xgb/'+files[0])\n",
    "    pred_prob = pred.pred_prob\n",
    "    for f in files[1:]:\n",
    "        pred = pd.read_csv('./preds_xgb/'+f)\n",
    "        pred_prob += pred.pred_prob\n",
    "\n",
    "    pred_prob /= len(files)\n",
    "\n",
    "    pred = pd.DataFrame(cust_id,columns=['cust_id']).reset_index(drop = True)\n",
    "    pred['pred_prob'] = pred_prob\n",
    "    pred.to_csv('./preds_xgb/avg_preds.csv',index=False)\n",
    "    return np.mean(auc_score)\n",
    "xgb(all_data_raw,train_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#0.75**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape (15000, 157)\n",
      "test_shape (10000, 157)\n",
      "###################### train!!! ################################\n",
      "[0]\tvalidation_0-auc:0.720102\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.794419\n",
      "[200]\tvalidation_0-auc:0.798787\n",
      "[300]\tvalidation_0-auc:0.803119\n",
      "[400]\tvalidation_0-auc:0.806547\n",
      "[500]\tvalidation_0-auc:0.80893\n",
      "[600]\tvalidation_0-auc:0.810614\n",
      "Stopping. Best iteration:\n",
      "[562]\tvalidation_0-auc:0.811092\n",
      "\n",
      "###################### valid!!! ##################################\n",
      "The auc_score is 0.8110920068976264\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "[0]\tvalidation_0-auc:0.690017\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.786046\n",
      "[200]\tvalidation_0-auc:0.794768\n",
      "[300]\tvalidation_0-auc:0.802211\n",
      "[400]\tvalidation_0-auc:0.805401\n",
      "[500]\tvalidation_0-auc:0.808161\n",
      "[600]\tvalidation_0-auc:0.807746\n",
      "Stopping. Best iteration:\n",
      "[518]\tvalidation_0-auc:0.808726\n",
      "\n",
      "###################### valid!!! ##################################\n",
      "The auc_score is 0.8087255289196771\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "[0]\tvalidation_0-auc:0.740177\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.820932\n",
      "[200]\tvalidation_0-auc:0.819686\n",
      "Stopping. Best iteration:\n",
      "[125]\tvalidation_0-auc:0.82216\n",
      "\n",
      "###################### valid!!! ##################################\n",
      "The auc_score is 0.8221599368030869\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "[0]\tvalidation_0-auc:0.731728\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.828718\n",
      "[200]\tvalidation_0-auc:0.831186\n",
      "[300]\tvalidation_0-auc:0.835513\n",
      "[400]\tvalidation_0-auc:0.839299\n",
      "[500]\tvalidation_0-auc:0.83919\n",
      "Stopping. Best iteration:\n",
      "[406]\tvalidation_0-auc:0.839648\n",
      "\n",
      "###################### valid!!! ##################################\n",
      "The auc_score is 0.8396479607855052\n",
      "###################### predict!!! ################################\n",
      "###################### train!!! ################################\n",
      "[0]\tvalidation_0-auc:0.728389\n",
      "Will train until validation_0-auc hasn't improved in 100 rounds.\n",
      "[100]\tvalidation_0-auc:0.792105\n",
      "[200]\tvalidation_0-auc:0.800977\n",
      "[300]\tvalidation_0-auc:0.804414\n",
      "[400]\tvalidation_0-auc:0.809213\n",
      "[500]\tvalidation_0-auc:0.811708\n",
      "[600]\tvalidation_0-auc:0.813132\n",
      "[700]\tvalidation_0-auc:0.814378\n",
      "[800]\tvalidation_0-auc:0.814109\n",
      "Stopping. Best iteration:\n",
      "[716]\tvalidation_0-auc:0.81496\n",
      "\n",
      "###################### valid!!! ##################################\n",
      "The auc_score is 0.8149603108267608\n",
      "###################### predict!!! ################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8193171488465312"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def xgb(all_data,train_y):\n",
    "    \n",
    "    \n",
    "    if os.path.exists('preds_xgb') == True:\n",
    "        shutil.rmtree('preds_xgb')\n",
    "    os.mkdir('preds_xgb')\n",
    "    \n",
    "    train_x,train_unlabeled,test_x = split_all_data(all_data)\n",
    "    train_x=train_x.replace(np.nan,-99)\n",
    "    test_x = test_x.replace(np.nan,-99)\n",
    "    cust_id = test_x['cust_id']\n",
    "    auc_score = []\n",
    "    k = 5\n",
    "    skf = StratifiedKFold(n_splits=k,shuffle=True,random_state=1)\n",
    "    feats = [feature for feature in train_x.columns.values if feature not in ['cust_id','cust_group','y','belong']]\n",
    "    train_x = train_x[feats]\n",
    "    test_x = test_x[feats]\n",
    "    print('train_shape',train_x.shape)\n",
    "    print('test_shape',test_x.shape)\n",
    "    for k,(train_k,valid_k) in enumerate(skf.split(train_x,train_y)):\n",
    "        x_train,y_train,x_valid,y_valid = np.array(train_x)[train_k], np.array(train_y)[train_k], np.array(train_x)[valid_k], np.array(train_y)[valid_k]\n",
    "        print('###################### train!!! ################################')       \n",
    "        #model_xgb = XGBRegressor(n_estimators=700,max_depth=3,learning_rate=0.07, subsample=0.9,colsample_bytree=0.7)\n",
    "        model = XGBRegressor(learning_rate=0.01,\n",
    "                             n_estimators=10000,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=6,\n",
    "                             gamma=0,\n",
    "                             subsample=0.6,                  #这个参数控制对于每棵树，随机采样的比例。减小这个参数的值算法会更加保守，避免过拟合。\n",
    "                                                             #但是这个值设置的过小，它可能会导致欠拟合。典型值：0.5-1 \n",
    "                             colsample_bytree=0.6,           # 用来控制每颗树随机采样的列数的占比每一列是一个特征0.5-1\n",
    "                             objective= 'binary:logistic',\n",
    "                             reg_alpha = 0.01,\n",
    "                             seed=27)#XGBClassifierXGBRegressor\n",
    "#         param_test = {#'max_depth':list(range(3,30,3)), \n",
    "# #                       'min_child_weight':[list(range(2,10,2))],\n",
    "#                       'n_estimators': list(range(100,5000,500)),  \n",
    "#                       'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "#                       'gamma': [i/10.0 for i in range(0,5)],\n",
    "#                       'subsample':[i/10.0 for i in range(6,10)],\n",
    "#                       'colsample_bytree':[i/10.0 for i in range(6,10)],\n",
    "#                       'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "#                      }\n",
    "        #gsearch = GridSearchCV(estimator = model, param_grid = param_test, scoring='roc_auc', cv=5)\n",
    "        model.fit(x_train,y_train,\\\n",
    "                eval_set=[(x_valid, y_valid)],\\\n",
    "                eval_metric='auc',\\\n",
    "                early_stopping_rounds=100,\\\n",
    "                verbose = 100)\n",
    "        #model.fit(x_train,y_train)\n",
    "        \n",
    "        print('###################### valid!!! ##################################')\n",
    "        y_pred_val = model.predict(x_valid)\n",
    "        auc = roc_auc_score(y_valid,y_pred_val)\n",
    "        print('The auc_score is',auc)\n",
    "        auc_score.append(auc)\n",
    "        print('###################### predict!!! ################################')\n",
    "        test_pred_y = model.predict(np.array(test_x))\n",
    "        test_result = pd.DataFrame(columns=[\"cust_id\",\"pred_prob\"])\n",
    "        test_result.cust_id = cust_id\n",
    "        test_result.pred_prob = test_pred_y\n",
    "        test_result.to_csv(\"./preds_xgb/xgb{0}.csv\".format(k),index=None,encoding='utf-8')\n",
    "    #pred 取平均   \n",
    "    files = os.listdir('./preds_xgb')\n",
    "    pred = pd.read_csv('./preds_xgb/'+files[0])\n",
    "    pred_prob = pred.pred_prob\n",
    "    for f in files[1:]:\n",
    "        pred = pd.read_csv('./preds_xgb/'+f)\n",
    "        pred_prob += pred.pred_prob\n",
    "\n",
    "    pred_prob /= len(files)\n",
    "\n",
    "    pred = pd.DataFrame(cust_id,columns=['cust_id']).reset_index(drop = True)\n",
    "    pred['pred_prob'] = pred_prob\n",
    "    pred.to_csv('./preds_xgb/avg_preds.csv',index=False)\n",
    "    return np.mean(auc_score)\n",
    "xgb(all_data_raw,train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.80881, std: 0.04881, params: {'max_depth': 1, 'min_child_weight': 1},\n",
       "  mean: 0.80923, std: 0.04856, params: {'max_depth': 1, 'min_child_weight': 2},\n",
       "  mean: 0.80885, std: 0.04825, params: {'max_depth': 1, 'min_child_weight': 3},\n",
       "  mean: 0.81736, std: 0.04535, params: {'max_depth': 2, 'min_child_weight': 1},\n",
       "  mean: 0.81621, std: 0.04483, params: {'max_depth': 2, 'min_child_weight': 2},\n",
       "  mean: 0.81540, std: 0.04587, params: {'max_depth': 2, 'min_child_weight': 3},\n",
       "  mean: 0.81510, std: 0.04394, params: {'max_depth': 3, 'min_child_weight': 1},\n",
       "  mean: 0.81408, std: 0.04403, params: {'max_depth': 3, 'min_child_weight': 2},\n",
       "  mean: 0.81567, std: 0.04442, params: {'max_depth': 3, 'min_child_weight': 3}],\n",
       " {'max_depth': 2, 'min_child_weight': 1},\n",
       " 0.81736234004312)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':[1,2,3],\n",
    " 'min_child_weight':[1,2,3]\n",
    "}\n",
    "train_x,train_unlabeled,test_x = split_all_data(all_data_raw)\n",
    "train_x=train_x.replace(np.nan,-99)\n",
    "test_x = test_x.replace(np.nan,-99)\n",
    "feats = [feature for feature in train_x.columns.values if feature not in ['cust_id','cust_group','y','belong']]\n",
    "train_x = train_x[feats]\n",
    "test_x = test_x[feats]\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, \n",
    "                                                  n_estimators=140, \n",
    "                                                  #max_depth=5,\n",
    "                                                  #min_child_weight=1, \n",
    "                                                  gamma=0, \n",
    "                                                  subsample=0.8,\n",
    "                                                  colsample_bytree=0.8,\n",
    "                                                  objective= 'binary:logistic', \n",
    "                                                  nthread=4,\n",
    "                                                  scale_pos_weight=1,\n",
    "                                                  seed=27), \n",
    "                                                  param_grid = param_test1,\n",
    "                                                  scoring='roc_auc',\n",
    "                                                  n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train_x,train_y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_,     gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
